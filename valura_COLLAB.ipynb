{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Note for Developers:\n",
        "\n",
        "For quicker access during development, the Gemini API key is currently exposed publicly. It will be secured and stored properly in a protected format soon.\n",
        "\n",
        "The NGROK link is also shared publicly only for this project‚Äôs preview purposes and will remain temporary.\n",
        "\n",
        "Please be aware that the Gemini API may occasionally fail due to free-tier rate limits.\n",
        "\n",
        "This setup can also be integrated with local LLMs (e.g., newly released GPT open-source models or DeepSeek) using frameworks like Transformers or Ollama for offline or hybrid processing.\n",
        "\n",
        "Thank you for your understanding."
      ],
      "metadata": {
        "id": "4nyHgOb1HvsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6DeJaJI2rlx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title INSTALL ALL PACKAGES üéì\n",
        "!pip install pyngrok streamlit\n",
        "!pip install docx\n",
        "!pip install -q -U google-genai\n",
        "# Install packages\n",
        "!pip install python-docx\n",
        "!pip install chromadb python-docx pdfplumber beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title DOWNLOAD ALL DATA OF ADGM üå¥\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# --- 1. Make two folders ---\n",
        "user_dir = \"/content/user_uploads\"\n",
        "ref_dir = \"/content/adgm_reference\"\n",
        "os.makedirs(user_dir, exist_ok=True)\n",
        "os.makedirs(ref_dir, exist_ok=True)\n",
        "\n",
        "# --- 2. List of user company files (uploaded for checking) ---\n",
        "user_files = {\n",
        "    # User's company docs\n",
        "    \"Articles_of_Association.docx\": \"https://assets.adgm.com/download/assets/adgm-ra-model-articles-private-company-limited-by-shares.docx/015402647f0111ef91cdea7ac70a8286\",\n",
        "    \"Register_of_Directors.docx\": \"https://assets.adgm.com/download/assets/Register-of-Directors-template-v1.docx/5fe5cdc26ba511ef92f68ef69f84fa1c\",\n",
        "    \"Register_of_Shareholders.docx\": \"https://assets.adgm.com/download/assets/Template_RegisterOfShareholder-v1-20220107.docx/8fa6bd545b0c11efb61ca6e4a17c9897\",\n",
        "    \"UBO_Declaration_Guidance.pdf\": \"https://assets.adgm.com/download/assets/Beneficial+Ownership+and+Control+Guidance+2021.pdf/628c9b6e6b9b11efbc29f277c33965bd\",\n",
        "    \"Employment_Contract_2024.docx\": \"https://assets.adgm.com/download/assets/ADGM+Standard+Employment+Contract+Template+-+ER+2024+(Feb+2025).docx/ee14b252edbe11efa63b12b3a30e5e3a\",\n",
        "    \"Employment_Contract_2019.docx\": \"https://assets.adgm.com/download/assets/ADGM+Standard+Employment+Contract+-+ER+2019+-+Short+Version+(May+2024).docx/33b57a92ecfe11ef97a536cc36767ef8\",\n",
        "    \"Resolution_for_Incorporation.docx\": \"https://assets.adgm.com/download/assets/adgm-ra-resolution-multiple-incorporate-shareholders-LTD-incorporation-v2.docx/186a12846c3911efa4e6c6223862cd87\",\n",
        "    \"Shareholder_Resolution_Amendment_Articles.docx\": \"https://assets.adgm.com/download/assets/Templates_SHReso_AmendmentArticles-v1-20220107.docx/97120d7c5af911efae4b1e183375c0b2?forcedownload=1\"\n",
        "}\n",
        "\n",
        "# --- 3. List of reference/guidance/checklist files ---\n",
        "ref_files = {\n",
        "    \"Checklist_Company_Setup.pdf\": \"https://www.adgm.com/documents/registration-authority/registration-and-incorporation/checklist/branch-non-financial-services-20231228.pdf\",\n",
        "    \"Checklist_Private_Company_Limited.pdf\": \"https://www.adgm.com/documents/registration-authority/registration-and-incorporation/checklist/private-company-limited-by-guarantee-non-financial-services-20231228.pdf\",\n",
        "    \"General_Incorporation.html\": \"https://www.adgm.com/registration-authority/registration-and-incorporation\",\n",
        "    \"Guidance_Policy.html\": \"https://www.adgm.com/legal-framework/guidance-and-policy-statements\",\n",
        "    \"Setting_Up.html\": \"https://www.adgm.com/setting-up\"\n",
        "}\n",
        "\n",
        "# --- 4. Simple download function ---\n",
        "def download_file(url, save_path):\n",
        "    try:\n",
        "        r = requests.get(url, stream=True)\n",
        "        r.raise_for_status()\n",
        "        with open(save_path, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"‚úÖ Downloaded: {os.path.basename(save_path)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed: {url} | Error: {e}\")\n",
        "\n",
        "# --- 5. Download all user docs ---\n",
        "for filename, url in user_files.items():\n",
        "    download_file(url, os.path.join(user_dir, filename))\n",
        "\n",
        "print(\"\\nüìÇ All user docs saved to:\", user_dir)\n",
        "\n",
        "# --- 6. Download all reference files ---\n",
        "for filename, url in ref_files.items():\n",
        "    # Save HTML as text\n",
        "    if filename.endswith(\".html\"):\n",
        "        try:\n",
        "            r = requests.get(url)\n",
        "            r.raise_for_status()\n",
        "            with open(os.path.join(ref_dir, filename), 'w', encoding='utf-8') as f:\n",
        "                f.write(r.text)\n",
        "            print(f\"‚úÖ Saved HTML page: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed HTML: {url} | Error: {e}\")\n",
        "    else:\n",
        "        download_file(url, os.path.join(ref_dir, filename))\n",
        "\n",
        "print(\"\\nüóÇÔ∏è Reference files saved to:\", ref_dir)\n",
        "print(\"\\nüö¶ Download complete! Now your folders are organized and easy for RAG and compliance checking.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7kGMSLqM3Cj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RUN THIS BEFORE CREATING RAG FOR FAKE DATA FOR NOW üéâ\n",
        "import docx\n",
        "from docx.shared import RGBColor\n",
        "import os\n",
        "\n",
        "# --- Set the file path (where Register_of_Directors is) ---\n",
        "file_path = '/content/user_uploads/Register_of_Directors.docx'\n",
        "\n",
        "# --- 1. Open existing docx file ---\n",
        "doc = docx.Document(file_path)\n",
        "\n",
        "# --- 2. Add a simple sample info in the first table (Director Details) ---\n",
        "\n",
        "for table in doc.tables:\n",
        "    # Find the table that matches Director info (by columns)\n",
        "    if len(table.columns) >= 9:  # Director info table has 9 columns\n",
        "        # Fill first data row with example director info\n",
        "        if len(table.rows) > 1:  # First row is header\n",
        "            cells = table.rows[1].cells\n",
        "            cells[0].text = \"Alex Lee\"\n",
        "            cells[1].text = \"12 Palm St\"\n",
        "            cells[2].text = \"Dubai\"\n",
        "            cells[3].text = \"UAE\"\n",
        "            cells[4].text = \"01/01/1980\"\n",
        "            cells[5].text = \"Emirati\"\n",
        "            cells[6].text = \"CEO\"\n",
        "            cells[7].text = \"01/01/2024\"\n",
        "            cells[8].text = \"01/05/2025\"\n",
        "\n",
        "            # (Optional) Add a second sample director in row 2\n",
        "            if len(table.rows) > 2:\n",
        "                cells = table.rows[2].cells\n",
        "                cells[0].text = \"Sara Patel\"\n",
        "                cells[1].text = \"45 Lake Rd\"\n",
        "                cells[2].text = \"Abu Dhabi\"\n",
        "                cells[3].text = \"UAE\"\n",
        "                cells[4].text = \"12/12/1990\"\n",
        "                cells[5].text = \"Indian\"\n",
        "                cells[6].text = \"Director\"\n",
        "                cells[7].text = \"02/02/2024\"\n",
        "                cells[8].text = \"‚Äî\"\n",
        "        break\n",
        "\n",
        "# --- 3. Add sample data in second table (Company/Firm info) ---\n",
        "for table in doc.tables:\n",
        "    # Find table with at least 10 columns (Company info)\n",
        "    if len(table.columns) >= 10:\n",
        "        if len(table.rows) > 1:\n",
        "            cells = table.rows[1].cells\n",
        "            cells[0].text = \"ADGM Tech Ltd\"\n",
        "            cells[1].text = \"ADGM Office\"\n",
        "            cells[2].text = \"CL12345\"\n",
        "            cells[3].text = \"Abu Dhabi, UAE\"\n",
        "            cells[4].text = \"LLC\"\n",
        "            cells[5].text = \"ADGM Law\"\n",
        "            cells[6].text = \"01/01/2024\"\n",
        "            cells[7].text = \"01/02/2024\"\n",
        "            cells[8].text = \"01/05/2025\"\n",
        "            cells[9].text = \"01/06/2025\"\n",
        "        break\n",
        "\n",
        "# --- 4. Save the updated docx! ---\n",
        "save_path = '/content/user_uploads/Register_of_Directors.docx'\n",
        "doc.save(save_path)\n",
        "print(f\"‚úÖ Saved file with sample data: {save_path}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0ZyOswqf3WxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CREATE CROMADB RAG WITH DATA AND TESTüëç\n",
        "import os\n",
        "import pdfplumber\n",
        "import docx\n",
        "import chromadb\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- SEPARATE FOLDERS FOR FILES ---\n",
        "USER_FOLDER = \"/content/user_uploads\"\n",
        "REF_FOLDER = \"/content/adgm_reference\"\n",
        "\n",
        "# --- ChromaDB Setup ---\n",
        "chroma_client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "collection = chroma_client.get_or_create_collection(name=\"adgm_rules\")\n",
        "\n",
        "# --- Functions to extract text from different file types ---\n",
        "def extract_text_from_pdf(path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def extract_text_from_docx(path):\n",
        "    doc = docx.Document(path)\n",
        "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "def extract_text_from_html(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        soup = BeautifulSoup(f.read(), \"html.parser\")\n",
        "    return soup.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "# --- Chunking function ---\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# --- Add only REFERENCE files to ChromaDB for rules ---\n",
        "doc_id = 0\n",
        "for file in os.listdir(REF_FOLDER):\n",
        "    file_path = os.path.join(REF_FOLDER, file)\n",
        "    if file.lower().endswith(\".pdf\"):\n",
        "        text = extract_text_from_pdf(file_path)\n",
        "    elif file.lower().endswith(\".docx\"):\n",
        "        text = extract_text_from_docx(file_path)\n",
        "    elif file.lower().endswith(\".html\"):\n",
        "        text = extract_text_from_html(file_path)\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    chunks = chunk_text(text)\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        collection.add(\n",
        "            documents=[chunk],\n",
        "            metadatas=[{\"source\": file, \"chunk_id\": idx}],\n",
        "            ids=[f\"{doc_id}_{idx}\"]\n",
        "        )\n",
        "    doc_id += 1\n",
        "\n",
        "print(\"‚úÖ Reference/checklist files loaded into ChromaDB\")\n",
        "\n",
        "# --- Query the vector DB (from reference, not uploads) ---\n",
        "query = \"Required documents for Company Incorporation\"\n",
        "results = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=3\n",
        ")\n",
        "\n",
        "print(\"\\nüîç Top Results for Query:\")\n",
        "for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
        "    print(f\"\\nüìÑ From: {meta['source']}\")\n",
        "    print(doc)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Graz3yUf3Nqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title save app.py first\n",
        "%%writefile app.py\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import docx\n",
        "from docx.shared import RGBColor\n",
        "import pdfplumber\n",
        "import streamlit as st\n",
        "import google.genai as genai\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "\n",
        "# ------------------------ SETUP FOLDERS ------------------------\n",
        "USER_FOLDER = \"/content/user_uploads\"\n",
        "REF_FOLDER = \"/content/adgm_reference\"\n",
        "REVIEWED_FOLDER = \"/content/reviewed_docs\"\n",
        "os.makedirs(USER_FOLDER, exist_ok=True)\n",
        "os.makedirs(REF_FOLDER, exist_ok=True)\n",
        "os.makedirs(REVIEWED_FOLDER, exist_ok=True)\n",
        "\n",
        "\n",
        "# ------------------------ AI SETUP ------------------------\n",
        "API_KEY = \"AIzaSyCw4-gMPk5P3LRahNjTwkZ6VBJ39v1hKEc\"\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "\n",
        "COMPANY_CHECKLIST = {\n",
        "    \"Company Incorporation\": [\n",
        "        \"Articles of Association\",\n",
        "        \"Board Resolution\",\n",
        "        \"Employment Contract\",\n",
        "        \"UBO Declaration\",\n",
        "        \"Register of Directors\",\n",
        "        \"Register of Shareholders\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "# ------------------------ HELPER FUNCTIONS ------------------------\n",
        "def read_pdf(file_path):\n",
        "    text_parts = []\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                text_parts.append(text)\n",
        "    return \"\\n\".join(text_parts)\n",
        "\n",
        "\n",
        "def read_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text_parts = []\n",
        "    for p in doc.paragraphs:\n",
        "        if p.text.strip():\n",
        "            text_parts.append(p.text)\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            row_text = \" | \".join(cell.text.strip() for cell in row.cells if cell.text.strip())\n",
        "            if row_text:\n",
        "                text_parts.append(row_text)\n",
        "    return \"\\n\".join(text_parts)\n",
        "\n",
        "\n",
        "def add_comment(paragraph, comment_text):\n",
        "    run = paragraph.add_run(f\"  [COMMENT: {comment_text}]\")\n",
        "    run.font.color.rgb = RGBColor(255, 0, 0)\n",
        "    run.bold = True\n",
        "\n",
        "\n",
        "def annotate_docx(doc_path, issues, output_path):\n",
        "    doc = docx.Document(doc_path)\n",
        "    for issue in issues:\n",
        "        found = False\n",
        "        for p in doc.paragraphs:\n",
        "            if issue[\"keyword\"].lower() in p.text.lower():\n",
        "                add_comment(p, issue[\"suggestion\"])\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            p = doc.add_paragraph()\n",
        "            add_comment(p, f\"(Not found in text) {issue['suggestion']}\")\n",
        "    doc.save(output_path)\n",
        "\n",
        "\n",
        "def parse_json_safe(raw_text):\n",
        "    try:\n",
        "        return json.loads(raw_text)\n",
        "    except json.JSONDecodeError:\n",
        "        match = re.search(r\"\\[.*\\]\", raw_text, re.S)\n",
        "        if match:\n",
        "            json_str = match.group(0)\n",
        "            json_str = re.sub(r\",\\s*}\", \"}\", json_str)\n",
        "            json_str = re.sub(r\",\\s*]\", \"]\", json_str)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except:\n",
        "                return []\n",
        "        return []\n",
        "\n",
        "\n",
        "# ------------------------ STREAMLIT APP STARTS ------------------------\n",
        "\n",
        "st.set_page_config(page_title=\"ADGM Compliance Checker\", layout=\"wide\")\n",
        "st.title(\"üéâ ADGM Company Compliance Checker\")\n",
        "st.markdown(\"See every step explained in **easy English with emojis!** üòÑ\")\n",
        "\n",
        "# ------------------------ FILE UPLOAD SECTION ------------------------\n",
        "st.header(\"üì§ Step 1: Upload Your Documents\")\n",
        "st.markdown(\"**Choose one way to add files:** ü§î\")\n",
        "\n",
        "# Create tabs for two options\n",
        "tab1, tab2 = st.tabs([\"üì§ Upload New Files\", \"üìÇ Use Existing Files\"])\n",
        "\n",
        "with tab1:\n",
        "    st.markdown(\"### üÜï Upload Your Documents Here\")\n",
        "    uploaded_files = st.file_uploader(\n",
        "        \"Choose your ADGM documents\",\n",
        "        type=['pdf', 'docx'],\n",
        "        accept_multiple_files=True,\n",
        "        help=\"üìã Upload PDF or DOCX files only\"\n",
        "    )\n",
        "\n",
        "    if uploaded_files:\n",
        "        st.success(f\"‚úÖ {len(uploaded_files)} files uploaded! üéâ\")\n",
        "\n",
        "        # Save uploaded files to USER_FOLDER\n",
        "        for uploaded_file in uploaded_files:\n",
        "            file_path = os.path.join(USER_FOLDER, uploaded_file.name)\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(uploaded_file.getbuffer())\n",
        "            st.markdown(f\"üíæ Saved: `{uploaded_file.name}`\")\n",
        "\n",
        "        st.markdown(\"**üí° Suggestion:** Files saved! Now scroll down to see the magic happen! ‚ú®\")\n",
        "\n",
        "with tab2:\n",
        "    st.markdown(\"### üìÅ Files Already in Folder\")\n",
        "    existing_files = os.listdir(USER_FOLDER)\n",
        "    if existing_files:\n",
        "        st.success(f\"‚úÖ Found {len(existing_files)} existing files! üìã\")\n",
        "        for file in existing_files:\n",
        "            st.markdown(f\"üìÑ `{file}`\")\n",
        "    else:\n",
        "        st.info(\"ü§∑‚Äç‚ôÇÔ∏è No files found in folder yet. Try uploading some above! ‚¨ÜÔ∏è\")\n",
        "\n",
        "st.markdown(\"---\")  # Nice separator line\n",
        "\n",
        "# ------------------------ STAGE 1: CLASSIFICATION & DETECTION ------------------------\n",
        "\n",
        "st.header(\"üé¨ Stage 1: Classification & Detection\")\n",
        "uploaded_types = []\n",
        "detected_types = {}\n",
        "\n",
        "user_files = os.listdir(USER_FOLDER)\n",
        "if not user_files:\n",
        "    st.warning(\"No files found in user uploads folder! Upload files above or place .docx or .pdf files in `/content/user_uploads`.\")\n",
        "else:\n",
        "    for file in user_files:\n",
        "        file_path = os.path.join(USER_FOLDER, file)\n",
        "        text = \"\"\n",
        "        if file.lower().endswith(\".docx\"):\n",
        "            text = read_docx(file_path)\n",
        "        elif file.lower().endswith(\".pdf\"):\n",
        "            text = read_pdf(file_path)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        detect_prompt = f\"\"\"\n",
        "        Classify the type of this legal document.\n",
        "        Choose only one (write just the name): Employment Contract, Articles of Association, Memorandum of Association, Board Resolution, UBO Declaration, Register of Directors, Register of Shareholders, Other.\n",
        "        Document text:\n",
        "        {text[:3000]}\n",
        "        \"\"\"\n",
        "        resp = client.models.generate_content(model=\"gemini-2.0-flash\", contents=detect_prompt)\n",
        "        doc_type = resp.text.strip()\n",
        "        uploaded_types.append(doc_type)\n",
        "        detected_types[file] = doc_type\n",
        "        st.markdown(f\"üü¶ **File:** `{file}`  ‚Üí  **Gemini detected type:** *{doc_type}*\")\n",
        "\n",
        "# ------------------------ STAGE 2: CHECKLIST MATCHING ------------------------\n",
        "\n",
        "st.header(\"üìã Stage 2: Checklist Matching\")\n",
        "def fuzzy_match(doc_type, required_type):\n",
        "    return required_type.lower() in doc_type.lower()\n",
        "\n",
        "missing_docs = []\n",
        "process = \"Company Incorporation\"\n",
        "required_docs = COMPANY_CHECKLIST[process]\n",
        "for required_doc in required_docs:\n",
        "    found = any(fuzzy_match(d_type, required_doc) for d_type in uploaded_types)\n",
        "    if not found:\n",
        "        missing_docs.append(required_doc)\n",
        "\n",
        "if len(uploaded_types) > 0:\n",
        "    st.markdown(f\"üü© **Required:** {len(required_docs)} | **Uploaded:** {len(uploaded_types)} | **Missing:** {len(missing_docs)}\")\n",
        "    if missing_docs:\n",
        "        st.error(f\"‚ùó Missing Documents: {missing_docs}\")\n",
        "    else:\n",
        "        st.success(\"‚úÖ All required documents are present! üéâ\")\n",
        "        st.markdown(\"**Pro Suggestion:** All documents detected. Next, check for mistakes and get suggestions below! üòÉ\")\n",
        "\n",
        "# ------------------------ STAGE 3: COMPLIANCE REVIEW & ANNOTATION ------------------------\n",
        "\n",
        "st.header(\"üìë Stage 3: Compliance Review & Annotation\")\n",
        "all_issues = []\n",
        "\n",
        "progress = st.progress(0)\n",
        "total_docs = len(list(detected_types.items()))\n",
        "current = 0\n",
        "\n",
        "for file, doc_type in detected_types.items():\n",
        "    file_path = os.path.join(USER_FOLDER, file)\n",
        "    if not file.lower().endswith(\".docx\"):\n",
        "        continue\n",
        "    document_text = read_docx(file_path)\n",
        "    retrieved_rules = \"ADGM official checklist details for \" + doc_type\n",
        "\n",
        "    review_prompt = f\"\"\"\n",
        "    You are an ADGM compliance assistant.\n",
        "    Review the following {doc_type} against ADGM official rules.\n",
        "    Return ONLY valid JSON in this exact format:\n",
        "    [\n",
        "      {{\n",
        "        \"document\": \"{doc_type}\",\n",
        "        \"section\": \"Clause X\",\n",
        "        \"keyword\": \"word from doc to locate issue\",\n",
        "        \"issue\": \"short description\",\n",
        "        \"severity\": \"High/Medium/Low\",\n",
        "        \"suggestion\": \"how to fix\"\n",
        "      }}\n",
        "    ]\n",
        "    ADGM Official Rules:\n",
        "    {retrieved_rules}\n",
        "    Uploaded Document:\n",
        "    {document_text[:4000]}\n",
        "    \"\"\"\n",
        "    review_resp = client.models.generate_content(model=\"gemini-2.0-flash\", contents=review_prompt)\n",
        "    issues_detected = parse_json_safe(review_resp.text)\n",
        "    annotated_path = os.path.join(REVIEWED_FOLDER, f\"Reviewed_{file}\")\n",
        "    current += 1\n",
        "    progress.progress(current / total_docs if total_docs else 1)\n",
        "\n",
        "    if issues_detected:\n",
        "        annotate_docx(file_path, issues_detected, annotated_path)\n",
        "        all_issues.extend(issues_detected)\n",
        "        with st.expander(f\"üìù Annotated and saved: {file}\", expanded=False):\n",
        "            for issue in issues_detected:\n",
        "                st.markdown(f\"üî• **{issue['keyword']}** (Severity: {issue['severity']}) ‚Üí _{issue['suggestion']}_\")\n",
        "    else:\n",
        "        st.success(f\"‚úÖ {file} looks clean! No Gemini problems found.\")\n",
        "\n",
        "# ---- DOWNLOAD BUTTONS for reviewed DOCX and ZIP ----\n",
        "import glob\n",
        "reviewed_files = [f for f in os.listdir(REVIEWED_FOLDER) if f.lower().endswith('.docx')]\n",
        "\n",
        "st.header(\"üì• Download Your Reviewed Files & Report\")\n",
        "for filename in reviewed_files:\n",
        "    file_path = os.path.join(REVIEWED_FOLDER, filename)\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        st.download_button(\n",
        "            label=f\"‚¨áÔ∏è Download {filename}\",\n",
        "            data=f,\n",
        "            file_name=filename,\n",
        "            mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
        "        )\n",
        "\n",
        "# ZIP Download button for ALL files\n",
        "if reviewed_files:\n",
        "    zip_buffer = io.BytesIO()\n",
        "    with zipfile.ZipFile(zip_buffer, \"w\") as zf:\n",
        "        for filename in reviewed_files:\n",
        "            file_path = os.path.join(REVIEWED_FOLDER, filename)\n",
        "            zf.write(file_path, arcname=filename)\n",
        "    zip_buffer.seek(0)\n",
        "    st.download_button(\n",
        "        label=\"üóÇÔ∏è Download ALL Reviewed DOCX as ZIP\",\n",
        "        data=zip_buffer,\n",
        "        file_name=\"ADGM_Reviewed_Documents.zip\",\n",
        "        mime=\"application/zip\",\n",
        "    )\n",
        "\n",
        "# Save and download JSON report\n",
        "final_report = {\n",
        "    \"process\": process,\n",
        "    \"documents_uploaded\": len(uploaded_types),\n",
        "    \"required_documents\": len(required_docs),\n",
        "    \"missing_documents\": missing_docs,\n",
        "    \"issues_found\": all_issues\n",
        "}\n",
        "\n",
        "with open(\"final_compliance_report.json\", \"w\") as f:\n",
        "    json.dump(final_report, f, indent=2)\n",
        "\n",
        "with open(\"final_compliance_report.json\", \"rb\") as f:\n",
        "    st.download_button(\"‚¨áÔ∏è Download JSON Report\", f.read(), \"final_compliance_report.json\", \"application/json\")\n",
        "\n",
        "# ------------------------ STAGE 4: SUMMARY & REPORT ------------------------\n",
        "\n",
        "st.header(\"üü™ Stage 4: Final Summary Report\")\n",
        "\n",
        "st.markdown(\"#### üìÅ The final compliance report:\")\n",
        "st.json(final_report)\n",
        "\n",
        "st.success(f\"‚úÖ Final JSON report saved at `/content/final_compliance_report.json`\")\n",
        "st.info(f\"üåü All reviewed (annotated) documents are saved in: `{REVIEWED_FOLDER}`\")\n",
        "\n",
        "if all_issues:\n",
        "    st.markdown(\"### üí° What to Fix (Suggestions):\")\n",
        "    for issue in all_issues[:8]:\n",
        "        st.markdown(f\"züîß {issue['suggestion']}\")\n",
        "\n",
        "st.markdown(\"> üòÑ **Super Example Output:** If you see all green ticks above, your company docs are perfect and ready for ADGM! üöÄ\")\n",
        "\n",
        "# ------------------------ END ------------------------\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.subheader(\"üåà Need Help?\")\n",
        "st.markdown(\"If you get any error, want to upload more files, or want a sample file, just ask! üëç\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y9dTX25H3zN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN STREAMLIT APP"
      ],
      "metadata": {
        "id": "jEH-BgSd5nx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Set Streamlit configA\n",
        "os.makedirs(\".streamlit\", exist_ok=True)\n",
        "with open(\".streamlit/config.toml\", \"w\") as f:\n",
        "    f.write(\"[server]\\nheadless = true\\nport = 5000\\nenableCORS = false\")\n",
        "\n",
        "# Start ngrok\n",
        "ngrok.set_auth_token(\"2UUGMJW8gaZ7Ikrl53By3xYHdLs_6b3ipRxC3rEXwy7JgQv5Y\")\n",
        "public_url = ngrok.connect(5000, domain=\"prepared-singularly-shepherd.ngrok-free.app\")\n",
        "print(f\"üåê Ngrok URL: {public_url}\")\n",
        "\n",
        "# Run Streamlit\n",
        "!streamlit run app.py --server.port 5000\n"
      ],
      "metadata": {
        "id": "w6Ewo3AA3Avf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LOGIC CODE WITHOUT STREAMLIT\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import docx\n",
        "from docx.shared import RGBColor\n",
        "import google.genai as genai\n",
        "import pdfplumber\n",
        "\n",
        "# --- Folder Setup ---\n",
        "USER_FOLDER = \"/content/user_uploads\"\n",
        "REF_FOLDER = \"/content/adgm_reference\"\n",
        "REVIEWED_FOLDER = \"/content/reviewed_docs\"  # üü© Your folder for annotated files!\n",
        "os.makedirs(USER_FOLDER, exist_ok=True)\n",
        "os.makedirs(REF_FOLDER, exist_ok=True)\n",
        "os.makedirs(REVIEWED_FOLDER, exist_ok=True)  # üü¢ Make sure this folder exists!\n",
        "\n",
        "# --- Gemini Setup ---\n",
        "API_KEY = \"AIzaSyAlmpL5nMrAWkTC_-xTDBs1Uo_A2dv8TBM\"\n",
        "client = genai.Client(api_key=API_KEY)\n",
        "\n",
        "# --- ADGM Checklist ---\n",
        "COMPANY_CHECKLIST = {\n",
        "    \"Company Incorporation\": [\n",
        "        \"Articles of Association\",\n",
        "        \"Board Resolution\",\n",
        "        \"Employment Contract\",\n",
        "        \"UBO Declaration\",\n",
        "        \"Register of Directors\",\n",
        "        \"Register of Shareholders\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def read_pdf(file_path):\n",
        "    text_parts = []\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                text_parts.append(text)\n",
        "    return \"\\n\".join(text_parts)\n",
        "\n",
        "def read_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    text_parts = []\n",
        "    for p in doc.paragraphs:\n",
        "        if p.text.strip():\n",
        "            text_parts.append(p.text)\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            row_text = \" | \".join(cell.text.strip() for cell in row.cells if cell.text.strip())\n",
        "            if row_text:\n",
        "                text_parts.append(row_text)\n",
        "    return \"\\n\".join(text_parts)\n",
        "\n",
        "def add_comment(paragraph, comment_text):\n",
        "    run = paragraph.add_run(f\"  [COMMENT: {comment_text}]\")\n",
        "    run.font.color.rgb = RGBColor(255, 0, 0)\n",
        "    run.bold = True\n",
        "\n",
        "def annotate_docx(doc_path, issues, output_path):\n",
        "    doc = docx.Document(doc_path)\n",
        "    for issue in issues:\n",
        "        found = False\n",
        "        for p in doc.paragraphs:\n",
        "            if issue[\"keyword\"].lower() in p.text.lower():\n",
        "                add_comment(p, issue[\"suggestion\"])\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            p = doc.add_paragraph()\n",
        "            add_comment(p, f\"(Not found in text) {issue['suggestion']}\")\n",
        "    doc.save(output_path)\n",
        "    print(f\"‚úÖ Annotated document saved: {output_path}\")\n",
        "\n",
        "def parse_json_safe(raw_text):\n",
        "    try:\n",
        "        return json.loads(raw_text)\n",
        "    except json.JSONDecodeError:\n",
        "        match = re.search(r\"\\[.*\\]\", raw_text, re.S)\n",
        "        if match:\n",
        "            json_str = match.group(0)\n",
        "            json_str = re.sub(r\",\\s*}\", \"}\", json_str)\n",
        "            json_str = re.sub(r\",\\s*]\", \"]\", json_str)\n",
        "            try:\n",
        "                return json.loads(json_str)\n",
        "            except:\n",
        "                return []\n",
        "        return []\n",
        "\n",
        "# ---------------- STAGE 1: Classification & Detection ----------------\n",
        "\n",
        "uploaded_types = []\n",
        "detected_types = {}\n",
        "\n",
        "print(\"üé¨ Stage 1: Classification & Detection\")\n",
        "for file in os.listdir(USER_FOLDER):\n",
        "    file_path = os.path.join(USER_FOLDER, file)\n",
        "    text = \"\"\n",
        "    if file.lower().endswith(\".docx\"):\n",
        "        text = read_docx(file_path)\n",
        "    elif file.lower().endswith(\".pdf\"):\n",
        "        text = read_pdf(file_path)\n",
        "    else:\n",
        "        continue  # Skip unknown file types\n",
        "\n",
        "    detect_prompt = f\"\"\"\n",
        "    Classify the type of this legal document.\n",
        "    Choose only one (write just the name): Employment Contract, Articles of Association, Memorandum of Association, Board Resolution, UBO Declaration, Register of Directors, Register of Shareholders,.\n",
        "    Document text:\n",
        "    {text[:3000]}\n",
        "    \"\"\"\n",
        "    resp = client.models.generate_content(model=\"gemini-2.0-flash\", contents=detect_prompt)\n",
        "    doc_type = resp.text.strip()\n",
        "    uploaded_types.append(doc_type)\n",
        "    detected_types[file] = doc_type\n",
        "    print(f\"üü¶ File: {file}  ‚Üí  Gemini detected type: {doc_type}\")\n",
        "\n",
        "# ---------------- STAGE 2: Checklist Matching ----------------\n",
        "\n",
        "def fuzzy_match(doc_type, required_type):\n",
        "    return required_type.lower() in doc_type.lower()\n",
        "\n",
        "missing_docs = []\n",
        "process = \"Company Incorporation\"\n",
        "required_docs = COMPANY_CHECKLIST[process]\n",
        "\n",
        "for required_doc in required_docs:\n",
        "    found = any(fuzzy_match(d_type, required_doc) for d_type in uploaded_types)\n",
        "    if not found:\n",
        "        missing_docs.append(required_doc)\n",
        "\n",
        "print(\"\\nüìã Stage 2: Checklist Matching\")\n",
        "print(f\"üü© Required: {len(required_docs)} | Uploaded: {len(uploaded_types)} | Missing: {len(missing_docs)}\")\n",
        "if missing_docs:\n",
        "    print(\"‚ùó Missing Documents:\", missing_docs)\n",
        "else:\n",
        "    print(\"‚úÖ All required documents are present! üéâ\")\n",
        "\n",
        "# ---------------- STAGE 3: Compliance Review & Annotation ----------------\n",
        "\n",
        "all_issues = []\n",
        "\n",
        "print(\"\\nüìë Stage 3: Annotating DOCX files for compliance\")\n",
        "for file, doc_type in detected_types.items():\n",
        "    file_path = os.path.join(USER_FOLDER, file)\n",
        "    if not file.lower().endswith(\".docx\"):\n",
        "        continue\n",
        "    document_text = read_docx(file_path)\n",
        "    retrieved_rules = \"ADGM official checklist details for \" + doc_type\n",
        "\n",
        "    review_prompt = f\"\"\"\n",
        "    You are an ADGM compliance assistant.\n",
        "    Review the following {doc_type} against ADGM official rules.\n",
        "    Return ONLY valid JSON in this exact format:\n",
        "    [\n",
        "      {{\n",
        "        \"document\": \"{doc_type}\",\n",
        "        \"section\": \"Clause X\",\n",
        "        \"keyword\": \"word from doc to locate issue\",\n",
        "        \"issue\": \"short description\",\n",
        "        \"severity\": \"High/Medium/Low\",\n",
        "        \"suggestion\": \"how to fix\"\n",
        "      }}\n",
        "    ]\n",
        "    ADGM Official Rules:\n",
        "    {retrieved_rules}\n",
        "\n",
        "    Uploaded Document:\n",
        "    {document_text[:4000]}\n",
        "    \"\"\"\n",
        "    review_resp = client.models.generate_content(model=\"gemini-2.0-flash\", contents=review_prompt)\n",
        "    issues_detected = parse_json_safe(review_resp.text)\n",
        "    annotated_path = os.path.join(REVIEWED_FOLDER, f\"Reviewed_{file}\")\n",
        "    if issues_detected:\n",
        "        annotate_docx(file_path, issues_detected, annotated_path)\n",
        "        all_issues.extend(issues_detected)\n",
        "        print(f\"üìù Annotated and saved: {annotated_path}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚úÖ {file} looks clean! No Gemini issues found.\")\n",
        "\n",
        "# ---------------- STAGE 4: Save and Print Final Report ----------------\n",
        "\n",
        "final_report = {\n",
        "    \"process\": process,\n",
        "    \"documents_uploaded\": len(uploaded_types),\n",
        "    \"required_documents\": len(required_docs),\n",
        "    \"missing_documents\": missing_docs,\n",
        "    \"issues_found\": all_issues\n",
        "}\n",
        "with open(\"/content/final_compliance_report.json\", \"w\") as f:\n",
        "    json.dump(final_report, f, indent=4)\n",
        "\n",
        "print(\"\\nüü™ Stage 4: Final Summary Report\")\n",
        "print(json.dumps(final_report, indent=2))\n",
        "print(\"‚úÖ Final JSON report saved at /content/final_compliance_report.json\")\n",
        "print(f\"üåü All reviewed documents saved in: {REVIEWED_FOLDER}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lfaKsqcD3bfj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}